{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f666baa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Toxic-Spans-Detection' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# clone our repository\n",
    "!git clone https://github.com/Azzam-Radman/Toxic-Spans-Detection.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0340787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from tqdm import tqdm # to show progress bar in for loops\n",
    "import pandas as pd # data reading and preprocessing library\n",
    "import numpy as np # math operations liabrary\n",
    "from nltk.tokenize import TweetTokenizer # tokenizer\n",
    "pd.set_option('display.max_colwidth', None) # to prevent the truncation of a cell content during display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83253424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Feed</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>اربد فيها جامعات اكثر من عمان ... وفيها قد عمان ونص لعيبه المنتخب منها ... و 80 % من مطربين الاردن منها</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>الحلو انكم بتحكوا على اساس انو الاردن ما فيه فساد سرقات</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>كله رائع بجد ربنا يكرمك</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>لسانك قذر يا قمامه</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>​انا داشره وغير متزوجه ولدي علاقات مشبوه واحشش واحيانا اهرب مخدرات و اجيد التسليك احب ان انكب نفسي وعلاقتي بالمنزل متوتره جد</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  \\\n",
       "0   1   \n",
       "1   2   \n",
       "2   3   \n",
       "3   4   \n",
       "4   5   \n",
       "\n",
       "                                                                                                                           Feed  \\\n",
       "0                       اربد فيها جامعات اكثر من عمان ... وفيها قد عمان ونص لعيبه المنتخب منها ... و 80 % من مطربين الاردن منها   \n",
       "1                                                                       الحلو انكم بتحكوا على اساس انو الاردن ما فيه فساد سرقات   \n",
       "2                                                                                                       كله رائع بجد ربنا يكرمك   \n",
       "3                                                                                                            لسانك قذر يا قمامه   \n",
       "4  ​انا داشره وغير متزوجه ولدي علاقات مشبوه واحشش واحيانا اهرب مخدرات و اجيد التسليك احب ان انكب نفسي وعلاقتي بالمنزل متوتره جد   \n",
       "\n",
       "  Sentiment  \n",
       "0  Positive  \n",
       "1  Negative  \n",
       "2  Positive  \n",
       "3  Negative  \n",
       "4  Negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "try:\n",
    "    # implemented if the repo is cloned\n",
    "    df = pd.read_csv('/content/Toxic-Spans-Detection/src/dataset/AJGT.csv')\n",
    "except:\n",
    "    # data read from the repo directly\n",
    "    df = pd.read_csv(r'https://raw.githubusercontent.com/Azzam-Radman/Toxic-Spans-Detection/main/src/dataset/AJGT.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7362b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    900\n",
       "Negative    900\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show count values of the label\n",
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f50856e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show shape of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bbb31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a tokenizer \n",
    "tknzr2 = TweetTokenizer()\n",
    "\n",
    "# function to tokenize a text in the form of a string\n",
    "def custom_tokenizer(text_data):\n",
    "    return tknzr2.tokenize(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66f34d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['اربد',\n",
       " 'فيها',\n",
       " 'جامعات',\n",
       " 'اكثر',\n",
       " 'من',\n",
       " 'عمان',\n",
       " '...',\n",
       " 'وفيها',\n",
       " 'قد',\n",
       " 'عمان',\n",
       " 'ونص',\n",
       " 'لعيبه',\n",
       " 'المنتخب',\n",
       " 'منها',\n",
       " '...',\n",
       " 'و',\n",
       " '80',\n",
       " '%',\n",
       " 'من',\n",
       " 'مطربين',\n",
       " 'الاردن',\n",
       " 'منها']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try tokening the first sentence in the dataset\n",
    "custom_tokenizer(df['Feed'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed5ba144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "# iterate over sentences to see the length of the longest sentence\n",
    "max_ = 0 # initialize max_ with 0\n",
    "for i in range(df.shape[0]):\n",
    "    len_ = len(custom_tokenizer(df['Feed'][i])) # the lenght of the sentence of row i\n",
    "    if len_ > max_:\n",
    "        max_ = len_\n",
    "\n",
    "# print the length of the longest sentence\n",
    "print(max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecd741d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  133  134  135  136  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   137  138  139  140  141  142  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 143 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct a placeholder ----> a dataframe with all zeors (# rows = the original dataset number of rows,\n",
    "# number of columnst = the longest sentence)\n",
    "tokenized_df = pd.DataFrame(np.zeros((df.shape[0], 143)), columns=[f\"{i}\" for i in range(143)])\n",
    "tokenized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f29e30eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1800/1800 [00:37<00:00, 47.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# loop over each sentence, tokenize it and pad the remaining of the max lenght (143) with a \"pad\" token\n",
    "for i in tqdm(range(df.shape[0])):\n",
    "    len_ = len(custom_tokenizer(df['Feed'][i]))\n",
    "    list_of_tokens = custom_tokenizer(df['Feed'][i])\n",
    "    list_of_tokens += ['pad'] * (143 - len_)\n",
    "    tokenized_df.iloc[i, :] = list_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57e3a8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اربد</td>\n",
       "      <td>فيها</td>\n",
       "      <td>جامعات</td>\n",
       "      <td>اكثر</td>\n",
       "      <td>من</td>\n",
       "      <td>عمان</td>\n",
       "      <td>...</td>\n",
       "      <td>وفيها</td>\n",
       "      <td>قد</td>\n",
       "      <td>عمان</td>\n",
       "      <td>...</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>الحلو</td>\n",
       "      <td>انكم</td>\n",
       "      <td>بتحكوا</td>\n",
       "      <td>على</td>\n",
       "      <td>اساس</td>\n",
       "      <td>انو</td>\n",
       "      <td>الاردن</td>\n",
       "      <td>ما</td>\n",
       "      <td>فيه</td>\n",
       "      <td>فساد</td>\n",
       "      <td>...</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>كله</td>\n",
       "      <td>رائع</td>\n",
       "      <td>بجد</td>\n",
       "      <td>ربنا</td>\n",
       "      <td>يكرمك</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>...</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>لسانك</td>\n",
       "      <td>قذر</td>\n",
       "      <td>يا</td>\n",
       "      <td>قمامه</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>...</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1       2      3      4     5       6      7    8     9  ...  \\\n",
       "0   اربد  فيها  جامعات   اكثر     من  عمان     ...  وفيها   قد  عمان  ...   \n",
       "1  الحلو  انكم  بتحكوا    على   اساس   انو  الاردن     ما  فيه  فساد  ...   \n",
       "2    كله  رائع     بجد   ربنا  يكرمك   pad     pad    pad  pad   pad  ...   \n",
       "3  لسانك   قذر      يا  قمامه    pad   pad     pad    pad  pad   pad  ...   \n",
       "\n",
       "   133  134  135  136  137  138  139  140  141  142  \n",
       "0  pad  pad  pad  pad  pad  pad  pad  pad  pad  pad  \n",
       "1  pad  pad  pad  pad  pad  pad  pad  pad  pad  pad  \n",
       "2  pad  pad  pad  pad  pad  pad  pad  pad  pad  pad  \n",
       "3  pad  pad  pad  pad  pad  pad  pad  pad  pad  pad  \n",
       "\n",
       "[4 rows x 143 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the head of the dataset\n",
    "tokenized_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b1aed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a new index ---> to the rows\n",
    "# this benificial in adding a new empty row under each original row \n",
    "# this empty row will be used for hand labelling\n",
    "new_idx = list(range(0, 1800*2, 2))\n",
    "tokenized_df.index = new_idx\n",
    "# reindex and add a new empty row after each original row\n",
    "tokenized_df_2 = tokenized_df.reindex(range(0, 3601, 1))\n",
    "tokenized_df_2 = tokenized_df_2.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ccde8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اربد</td>\n",
       "      <td>فيها</td>\n",
       "      <td>جامعات</td>\n",
       "      <td>اكثر</td>\n",
       "      <td>من</td>\n",
       "      <td>عمان</td>\n",
       "      <td>...</td>\n",
       "      <td>وفيها</td>\n",
       "      <td>قد</td>\n",
       "      <td>عمان</td>\n",
       "      <td>...</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الحلو</td>\n",
       "      <td>انكم</td>\n",
       "      <td>بتحكوا</td>\n",
       "      <td>على</td>\n",
       "      <td>اساس</td>\n",
       "      <td>انو</td>\n",
       "      <td>الاردن</td>\n",
       "      <td>ما</td>\n",
       "      <td>فيه</td>\n",
       "      <td>فساد</td>\n",
       "      <td>...</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>كله</td>\n",
       "      <td>رائع</td>\n",
       "      <td>بجد</td>\n",
       "      <td>ربنا</td>\n",
       "      <td>يكرمك</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>...</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>لسانك</td>\n",
       "      <td>قذر</td>\n",
       "      <td>يا</td>\n",
       "      <td>قمامه</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>...</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "      <td>pad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1       2      3      4     5       6      7    8     9  ...  \\\n",
       "0   اربد  فيها  جامعات   اكثر     من  عمان     ...  وفيها   قد  عمان  ...   \n",
       "1                                                                     ...   \n",
       "2  الحلو  انكم  بتحكوا    على   اساس   انو  الاردن     ما  فيه  فساد  ...   \n",
       "3                                                                     ...   \n",
       "4    كله  رائع     بجد   ربنا  يكرمك   pad     pad    pad  pad   pad  ...   \n",
       "5                                                                     ...   \n",
       "6  لسانك   قذر      يا  قمامه    pad   pad     pad    pad  pad   pad  ...   \n",
       "7                                                                     ...   \n",
       "\n",
       "   133  134  135  136  137  138  139  140  141  142  \n",
       "0  pad  pad  pad  pad  pad  pad  pad  pad  pad  pad  \n",
       "1                                                    \n",
       "2  pad  pad  pad  pad  pad  pad  pad  pad  pad  pad  \n",
       "3                                                    \n",
       "4  pad  pad  pad  pad  pad  pad  pad  pad  pad  pad  \n",
       "5                                                    \n",
       "6  pad  pad  pad  pad  pad  pad  pad  pad  pad  pad  \n",
       "7                                                    \n",
       "\n",
       "[8 rows x 143 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_df_2.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b40c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset as a CSV file\n",
    "tokenized_df_2.to_csv('tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293a3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
